# This R environment comes with all of CRAN preinstalled, as well as many other helpful packages
# The environment is defined by the kaggle/rstats docker image: https://github.com/kaggle/docker-rstats
# For example, here's several helpful packages to load in
library(ggplot2) # Data visualization
library(readr) # CSV file I/O, e.g. the read_csv function
# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory
system("ls ../input")
Kaggle_demonetization_tweets <- read_csv("../input/demonetization-tweets.csv",
col_types = cols(id = col_character(),
replyToSID = col_character()))
# Any results you write to the current directory are saved as output.
library(plotly)			# ggplot and dependencies
library(lubridate)		# Timezone conversion
library(scales)			# Regularly spaced dates (date_breaks)
library(dplyr)
library(stringr)
# Strip the timestamp into components to see the distribution of tweets over time
Kaggle_demonetization_tweets$created <- ymd_hms(Kaggle_demonetization_tweets$created)
# Change to India Timezone from UTC as the user activity seems to be peaking during odd hours otherwise
Kaggle_demonetization_tweets$created <- with_tz(Kaggle_demonetization_tweets$created, "Asia/Colombo")
# Frequency plot
ggplot(data = Kaggle_demonetization_tweets, aes(x = Kaggle_demonetization_tweets$created)) +
geom_histogram(aes(fill = ..count..), bins=60) +
xlab("Time") + ylab("Number of tweets") + labs(title = "Distribution of Tweets over entire duration") +
scale_fill_gradient(low = "lightblue", high = "darkgreen")
# Distribution of tweets by hour of the day (24hr period). I.e. Ignoring number of days, say at 4pm, 5pm, 6pm, etc. how many tweets were there?
Kaggle_demonetization_tweets$timeonly <- as.numeric(Kaggle_demonetization_tweets$created - trunc(Kaggle_demonetization_tweets$created, "days"))
class(Kaggle_demonetization_tweets$timeonly) <- "POSIXct"
Kaggle_demonetization_tweets$timeonly <- with_tz(Kaggle_demonetization_tweets$timeonly, "Asia/Colombo")
ggplot(data = Kaggle_demonetization_tweets, aes(x = timeonly)) +
geom_histogram(aes(fill = ..count..), bins = 25) +
xlab("Time") + ylab("Number of tweets") + labs(title = "Distribution of Tweets over time") +
scale_x_datetime(breaks = date_breaks("3 hours"), labels = date_format("%H:00")) +
scale_fill_gradient(low = "orange", high = "darkblue")
# Distribution of Tweets over time of day with "isRetweet" overlay
ggplot(Kaggle_demonetization_tweets, aes(timeonly)) +
geom_density(aes(fill = isRetweet), alpha = .5) +
scale_fill_discrete(guide = 'none') +
scale_x_datetime(breaks = date_breaks("3 hours"), labels = date_format("%H:00")) +
xlab('All tweets') + labs(title = "Distribution of Tweets over time of day")
# Twitter handles with maximum posts (TOP 10)
# Cleanup all URLs in statusSource column. In other words, extract all characters between > and <
Kaggle_demonetization_tweets$statusSource <- gsub("http[^[:blank:]]+", " ", Kaggle_demonetization_tweets$statusSource)
Kaggle_demonetization_tweets$statusSource <- gsub("[[:punct:]]", " ", Kaggle_demonetization_tweets$statusSource)
Kaggle_demonetization_tweets$statusSource <- gsub(" a ", " ", Kaggle_demonetization_tweets$statusSource)
Kaggle_demonetization_tweets$statusSource <- gsub("[ \t]{2,}", " ", Kaggle_demonetization_tweets$statusSource)
Kaggle_demonetization_tweets$statusSource <- gsub("^\\s+|\\s+$", " ", Kaggle_demonetization_tweets$statusSource)
Kaggle_demonetization_tweets$statusSource <- gsub(" a href rel nofollow ", " ", Kaggle_demonetization_tweets$statusSource)
Kaggle_demonetization_tweets$statusSource <- gsub(" href rel nofollow ", " ", Kaggle_demonetization_tweets$statusSource)
tweetsBySource <- Kaggle_demonetization_tweets %>%
group_by(statusSource) %>%
summarize(freqSrc=n()) %>%
arrange(desc(freqSrc))
tweetsBySource.Top <- tweetsBySource[order(-tweetsBySource$freqSrc),]
tweetsBySource.Top7 <- tweetsBySource.Top[1:7,]
ggplot(tweetsBySource.Top7, aes(sort(tweetsBySource.Top7$statusSource,decreasing = T),tweetsBySource.Top7$freqSrc)) +
geom_bar(stat = "identity", width=0.5) + coord_flip() +
geom_text(aes(label=tweetsBySource.Top7$freqSrc), hjust = -0.3, size =3, color = "red") +
ylab("Number of Tweets") + xlab("Twitter handle") + labs(title = "Top 7 Sources that contribute to maximum posts") +
theme(axis.text=element_text(size=10),
axis.title=element_text(size=16, colour = "blue"))
library(readr)
library(ggplot2)
library(forecast)
library(fpp2)
library(TTR)
library(dplyr)
install.packages(c("library(readr)", "library(ggplot2)", "library(forecast)", "library(fpp2)", "library(TTR)", "library(dplyr)"))
library(readr)
library(readr)
install.packages("digest")
install.packages(readr)
install.packages("tidyverse")
library(ggplot2)
library(readr)
library(readr)
install.packages("tidyverse")
library(readr)
library(ggplot2)
library(forecast)
library(fpp2)
library(TTR)
library(dplyr)
dat <- read_csv("timeseriesdata.csv")
glimpse(dat)
naive_mod <- naive(dat_ts, h = 12)
summary(naive_mod)
se_model <- ses(dat_ts, h = 12)
summary(se_model)
install.packages('ps')
library(ggplot2)
library("ggplot2")
library(readr)
library(ggplot2)
library(forecast)
library(fpp2)
library(TTR)
library(dplyr)
dat <- read_csv("timeseriesdata.csv")
glimpse(dat)
library(ggplot2)
library(tidyverse)
setwd("~/GitHub/fall20-bariskuyucu/files")
library(ggplot2)
install.packages(c("readr", "ggplot2", "forecast", "fpp2", "TTR", "dplyr"))
library(readr)
library(readr)
library(readr)
dat <- read_csv("timeseriesdata.csv")
library(readr)
library(readr)
library(knitr)
library(readr)
library(ggplot2)
library(forecast)
library(fpp2)
library(TTR)
library(dplyr)
dat <- read_csv("timeseriesdata.csv")
glimpse(dat)
glimpse(dat)
dat <- read_csv("timeseriesdata.csv")
glimpse(dat)
install.packages("timeseriesdb")
snaive(y, h = 2 * frequency(x))
summary(fcbeer)
daily_data = read.csv('day.csv', header=TRUE, stringsAsFactors=FALSE)
ggplot(daily_data, aes(Date, cnt)) + geom_line() + scale_x_date('month')  + ylab("Daily Bike Checkouts") +
xlab("")
ggplot() +
geom_line(data = daily_data, aes(x = Date, y = clean_cnt)) + ylab('Cleaned Bicycle Count')
count_ts = ts(daily_data[, c('cnt')])
daily_data$clean_cnt = tsclean(count_ts)
ggplot() +
geom_line(data = daily_data, aes(x = Date, y = clean_cnt)) + ylab('Cleaned Bicycle Count')
daily_data = read.csv('day.csv', header=TRUE, stringsAsFactors=FALSE)
daily_data = read.csv('day.csv', header=TRUE, stringsAsFactors=FALSE)
wines <- read.csv(file="AustralianWines.csv")
---
title: "Interesting Forecasting Applications with R"
author: "Ekrem Barış Kuyucu - IE360 - Fall 2020"
---
redwine.ts <- ts(wines$Red, start=c(1980,1), end=c(1994,12), frequency = 12)
time <- time(redwine.ts)
n.valid <- 2
n.train <- length(redwine.ts) - n.valid
redwine.train.ts <- window(redwine.ts, start=time[1], end=time[n.train])
redwine.valid.ts <- window(redwine.ts, start=time[n.train+1], end=time[n.train+n.valid])
naive.rose <- naive(rosewine.train.ts, h=2)
naive.rose$mean
##      Nov Dec
## 1994  57  57
snaive.rose <- snaive(rosewine.train.ts, h=2)
snaive.rose$mean
##      Nov Dec
## 1994  56  78
